% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/generateR.R
\name{generateR}
\alias{generateR}
\title{Generate fetch list of Url's from crawlDB}
\usage{
generateR(
  out_dir = NULL,
  work_dir = NULL,
  regExOut = NULL,
  regExIn = NULL,
  max_depth = NULL,
  topN = NULL,
  external_site = F,
  max_host = NULL,
  max_urls_per_host = 10,
  crawl_delay = NULL,
  log_file = NULL,
  seeds_only = F,
  min_score = 0
)
}
\arguments{
\item{out_dir}{(Required) Output directory for this crawl.}

\item{work_dir}{(Required) Working directory for this crawl.}

\item{regExOut}{RegEx URL filter - omit links with these keywords.}

\item{regExIn}{RegEx URL filter - keep links with these keywords.}

\item{max_depth}{maximum depth for selected url's}

\item{topN}{Choose these top links.}

\item{external_site}{Logical. If False, host outside the seed list will NOT be crawled.}

\item{max_urls_per_host}{Max URL's to generate per host.}

\item{crawl_delay}{crawl delay for requests to the same host}

\item{log_file}{Name of log file. If null, writes to stdout().}

\item{seeds_only}{gen only seeds}

\item{min_score}{minimum score for url}
}
\description{
Queries the crawlDB for urls matching the given parameters.
}
