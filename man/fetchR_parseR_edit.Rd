% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fetchR_parseR_edit.R
\name{fetchR_parseR_edit}
\alias{fetchR_parseR_edit}
\title{Fetch a List of Url's.}
\usage{
fetchR_parseR_edit(
  out_dir = NULL,
  work_dir = NULL,
  fetch_list = NULL,
  crawl_delay = NULL,
  max_concurr = NULL,
  max_concurr_host = NULL,
  timeout = Inf,
  timeout_request = NULL,
  queue_scl = 1,
  comments = "",
  log_file = NULL,
  readability_content = F,
  parser = crawlR::parse_content,
  writer = NULL,
  status_print_interval = 500,
  curl_opts = list(`User-Agent` =
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.117 Safari/537.36",
    `Accept-Language` = "en;q=0.7", Connection = "close", CURLOPT_DNS_CACHE_TIMEOUT =
    "3600")
)
}
\arguments{
\item{out_dir}{(Required) Current output directory.}

\item{work_dir}{(Required) Current working directory.}

\item{fetch_list}{(Required) Created by generateR.R.}

\item{crawl_delay}{time (in seconds) for calls to the same host.}

\item{max_concurr}{Max. total concurrent connections open at any given time.}

\item{max_concurr_host}{Max. total concurrent connections per host at any given time.}

\item{timeout}{Total (all requests) timeout}

\item{timeout_request}{per request timeout}

\item{queue_scl}{Scaler}

\item{comments}{Some comments to print while running.}

\item{log_file}{Name of log file. If null, writes to stdout().}

\item{readability_content}{T}

\item{parser}{parse func}

\item{writer}{placeholder to allow custom output functions}

\item{status_print_interval}{num urls fetched between crawler status outputs}

\item{curl_opts}{list of curl options}
}
\description{
Fetches list of URL's created by the generateR() function.
}
