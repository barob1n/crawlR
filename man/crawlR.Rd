% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/crawlR.R
\name{crawlR}
\alias{crawlR}
\title{Crawl and Parse Web Pages}
\usage{
crawlR(
  seeds = NULL,
  work_dir = NULL,
  out_dir = NULL,
  max_concurr = 50,
  max_host = 1,
  timeout = Inf,
  timeout_request = 30,
  external_site = F,
  crawl_delay = 30,
  max_size = 1e+07,
  regExIn = NULL,
  regExOut = NULL,
  depth = 1,
  max_depth = 3,
  queue_scl = 1,
  topN = NULL,
  max_urls_per_host = 10,
  n_threads = 1,
  parser = crawlR:::parse_content,
  score_func = NULL,
  log_file = NULL,
  seeds_only = F,
  crawl_int = NULL,
  readability_content = F,
  overwrite = F,
  min_score = 0
)
}
\arguments{
\item{seeds}{Seed url's. If null, then the work_dir must contain a linkDB.  If
additional seeds are provided after inital seeding, the new seed url's
will be added to linkDB and fetched.}

\item{work_dir}{(Required) Main working directory.}

\item{out_dir}{Directory to store crawled and parsed html If null defaults to work directory.}

\item{max_concurr}{Max. total concurrent connections open at any given time.}

\item{max_host}{Max. total concurrent connections per host at any given time.}

\item{timeout}{Total (as in all url's in seed list) time per each iteration (for each depth).}

\item{timeout_request}{per url timeout.}

\item{external_site}{If true, crawler will follow external links.}

\item{crawl_delay}{time (in seconds) for calls to the same host.
Only applies if the  time is not specified by the host's robots.txt.}

\item{max_size}{Max size of file or webpage to download and parse.}

\item{regExIn}{url's matching this regular expression will be used.}

\item{regExOut}{url's matching this reg-ex  will be filtered out, including url's that match regExIn.}

\item{depth}{Crawl depth for this crawl - A value of 1 only crawls the seed pages, 2 crawls links found on seeds, etc..}

\item{max_depth}{Where as the 'depth' variable determines the depth of the current crawl, 'max_depth' sets a maximum
overall depth so that no link with depth higher than this value will be selected for crawling during the generate phase.}

\item{queue_scl}{(deprecated) max_concur * queue_scl gives que.}

\item{topN}{Select the 'topN' links based on score for crawling.}

\item{max_urls_per_host}{Maximum url's from each host when creating fetch list for each link depth.}

\item{n_threads}{(depricated) Only applies to parsing.}

\item{parser}{Parsing function to use.}

\item{score_func}{URL Scoring Function.}

\item{log_file}{Name of log file. If null, writes to stdout().}

\item{seeds_only}{If true, only seeds will be pulled from linkDB.}

\item{readability_content}{process content using readability}

\item{overwrite}{If true, data for url will be overwritten in linkDB.}

\item{min_score}{minimum score during generate for urls}
}
\description{
Utilizes the curl package to crawl through a
list of user supplied websites to given depth.
Unlesss overriden, crawlR observes robots.txt.
}
\details{
After each iteration of crawling, the crawled
pages are read from disk, parsed, and writen
back to disk. The read/parse phase is done in
parrallel using the future package
}
\examples{


## SETUP --------------------------------------------------------------------

library(crawlR)

work_dir <- '/usr/local/solr/crawl/'
max_concurr = 50         # max concurrent connections - total
max_host = 1             # max concurrent connections - per host
crawl_delay = 30         # delay in seconds between sucessvie requests same host
timeout = Inf            # total time for crawling ALL urls


  fh <- file(paste0(work_dir,'/seeds.txt'))

  seeds <- readLines(fh)
  close(fh)

  ## Get initial pages
  crawlR::crawlR(
    seeds = seeds,
    work_dir = work_dir,
    out_dir =  out_dir,
    max_concurr = max_concurr,
    max_host = max_host,
    timeout = timeout,
    external_site = F,
    crawl_delay=crawl_delay,
    max_size = 4e6,
    regExOut = NULL,
    regExIn = NULL,
    depth = 1,
    queue_scl = 1,
    topN=NULL,
    max_urls_per_host = 1,
    n_threads = 2,
    parser = crawlR:::parse_content)

  file.remove(paste0(work_dir,'/seeds.txt'))


}
