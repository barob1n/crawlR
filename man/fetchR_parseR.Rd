% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fetchR_parseR.R
\name{fetchR_parseR}
\alias{fetchR_parseR}
\title{Fetch a List of Url's.}
\usage{
fetchR_parseR(
  out_dir = NULL,
  work_dir = NULL,
  fetch_list = NULL,
  crawl_delay = NULL,
  max_concurr = NULL,
  max_host = NULL,
  timeout = Inf,
  timeout_request = NULL,
  queue_scl = 1,
  comments = "",
  log_file = NULL,
  readability_content = F,
  parser = crawlR::parse_content_fetch
)
}
\arguments{
\item{out_dir}{(Required) Current output directory.}

\item{work_dir}{(Required) Current working directory.}

\item{fetch_list}{(Required) Created by generateR.R.}

\item{crawl_delay}{time (in seconds) for calls to the same host.}

\item{max_concurr}{Max. total concurrent connections open at any given time.}

\item{max_host}{Max. total concurrent connections per host at any given time.}

\item{timeout}{Total (all requests) timeout}

\item{timeout_request}{per request timeout}

\item{queue_scl}{Scaler}

\item{comments}{Some comments to print while running.}

\item{log_file}{Name of log file. If null, writes to stdout().}

\item{readability_content}{T}

\item{parser}{parse func}
}
\value{
None.
}
\description{
Based on the curl package (a wrapper for libcurl). The fetch list
of urls is organized into batches, with each batch containing one
url from one host. Provides a convienent way to avoid
hitting a server too often.  A delay also kicks in if a host is
being queried too quickly.
}
